{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14b777f",
   "metadata": {},
   "source": [
    "# Project Build Tracking Notebook\n",
    "\n",
    "This notebook records the structured evolution of the E-commerce Demand Forecasting project. Each section is designed to be reproducible and auditable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e324aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Notebook Metadata & Project Overview\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "project_name = \"E-commerce Demand Forecasting\"\n",
    "author = \"Zak\"\n",
    "created_at = datetime.utcnow().isoformat()\n",
    "objectives = [\n",
    "    \"Predict 30-day item-level demand\",\n",
    "    \"Compare classical vs deep sequence models\",\n",
    "    \"Deploy forecast API\",\n",
    "    \"Demonstrate reproducible ML pipeline\"\n",
    "]\n",
    "milestones = {\n",
    "    \"data_download\": False,\n",
    "    \"panel_prepared\": False,\n",
    "    \"features_built\": False,\n",
    "    \"baseline_trained\": False,\n",
    "    \"nbeats_trained\": False,\n",
    "    \"api_live\": False,\n",
    "}\n",
    "print(json.dumps({\n",
    "    \"project_name\": project_name,\n",
    "    \"author\": author,\n",
    "    \"created_at\": created_at,\n",
    "    \"objectives\": objectives,\n",
    "    \"milestones\": milestones\n",
    "}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b614ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Environment & Dependency Check\n",
    "import sys, platform\n",
    "from importlib import import_module\n",
    "\n",
    "critical = [\"pandas\", \"polars\", \"torch\", \"pytorch_lightning\", \"fastapi\", \"numpy\"]\n",
    "status = {}\n",
    "for pkg in critical:\n",
    "    try:\n",
    "        import_module(pkg)\n",
    "        status[pkg] = \"OK\"\n",
    "    except Exception as e:\n",
    "        status[pkg] = f\"MISSING: {e}\"  # log error\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Dependency status:\")\n",
    "for k, v in status.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Persistent Configuration Loader\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "CONFIG_PATH = Path(\"config.yaml\")\n",
    "if not CONFIG_PATH.exists():\n",
    "    CONFIG_PATH.write_text(\"\"\"default_model: nbeats\\nforecast_horizon: 30\\ntrain_epochs: 5\\n\"\"\")\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "def get_config(key, default=None):\n",
    "    return CONFIG.get(key, default)\n",
    "\n",
    "print(\"Loaded config:\", CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d60c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Automatic Notebook Versioning Snapshot\n",
    "import json, time\n",
    "SNAPSHOT_DIR = Path('.tracking/snapshots')\n",
    "SNAPSHOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot = {\n",
    "    'timestamp': time.time(),\n",
    "    'project_name': project_name,\n",
    "    'milestones': milestones,\n",
    "    'config': CONFIG,\n",
    "    'python_version': sys.version,\n",
    "}\n",
    "# incremental index\n",
    "existing = sorted(SNAPSHOT_DIR.glob('snapshot_*.json'))\n",
    "idx = len(existing) + 1\n",
    "snap_path = SNAPSHOT_DIR / f'snapshot_{idx:04d}.json'\n",
    "with open(snap_path, 'w') as f:\n",
    "    json.dump(snapshot, f, indent=2)\n",
    "print('Saved snapshot to', snap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Data/Artifact Directory Structure Initialization\n",
    "DIRS = [\n",
    "    Path('data/raw'), Path('data/processed'), Path('artifacts/models'),\n",
    "    Path('artifacts/figures'), Path('logs'), Path('reports')\n",
    "]\n",
    "for d in DIRS:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "print('Ensured directories:', [str(d) for d in DIRS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930685c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Requirements Freeze & Export\n",
    "import subprocess\n",
    "REQ_LOCK = Path('requirements.lock')\n",
    "new_freeze = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze']).decode().splitlines()\n",
    "old_freeze = []\n",
    "if REQ_LOCK.exists():\n",
    "    old_freeze = REQ_LOCK.read_text().splitlines()\n",
    "REQ_LOCK.write_text('\\n'.join(new_freeze))\n",
    "print('Wrote requirements.lock with', len(new_freeze), 'packages')\n",
    "added = set(new_freeze) - set(old_freeze)\n",
    "removed = set(old_freeze) - set(new_freeze)\n",
    "print('Added:', len(added), 'Removed:', len(removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Core Utility Functions Prototype\n",
    "from typing import Any\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def timestamp() -> str:\n",
    "    return datetime.utcnow().isoformat()\n",
    "\n",
    "def load_raw(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load a raw CSV if present else return mock dataframe.\"\"\"\n",
    "    if path.exists():\n",
    "        return pd.read_csv(path)\n",
    "    return pd.DataFrame({'id': [1,2,3], 'value': [10,11,12]})\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Simple placeholder cleaning: drop NA and enforce types.\"\"\"\n",
    "    return df.dropna()\n",
    "\n",
    "def save_processed(df: pd.DataFrame, path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(path, index=False)\n",
    "\n",
    "print('Utility functions defined at', timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Logging Setup & Inline Logger Test\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "LOG_PATH = Path('logs/app.log')\n",
    "LOG_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "handler = RotatingFileHandler(LOG_PATH, maxBytes=200_000, backupCount=3)\n",
    "logging.basicConfig(level=logging.INFO, handlers=[handler], format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger('build')\n",
    "logger.info('Logger initialized')\n",
    "logger.warning('Sample warning message')\n",
    "try:\n",
    "    raise ValueError('Simulated error for logging')\n",
    "except Exception as e:\n",
    "    logger.error('Captured exception: %s', e)\n",
    "print('Log file size bytes:', LOG_PATH.stat().st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c39336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Data Ingestion Placeholder Cell\n",
    "RAW_SAMPLE_PATH = Path('data/raw/sample.csv')\n",
    "if not RAW_SAMPLE_PATH.exists():\n",
    "    RAW_SAMPLE_PATH.write_text('id,value\\n1,10\\n2,11\\n3,12')\n",
    "raw_df = load_raw(RAW_SAMPLE_PATH)\n",
    "print('Raw DF shape:', raw_df.shape)\n",
    "print(raw_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Exploratory Scratchpad (Tracked)\n",
    "profile = {\n",
    "    'head': raw_df.head().to_dict(),\n",
    "    'describe': raw_df.describe().to_dict(),\n",
    "}\n",
    "ARTIFACT_PROFILE = Path('artifacts/profile_raw.json')\n",
    "ARTIFACT_PROFILE.write_text(json.dumps(profile, indent=2))\n",
    "print('Saved profile to', ARTIFACT_PROFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87cbfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Reusable Processing Pipeline Skeleton\n",
    "from typing import Callable, List\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.steps: List[Callable[[pd.DataFrame], pd.DataFrame]] = []\n",
    "    def add(self, fn: Callable[[pd.DataFrame], pd.DataFrame]):\n",
    "        self.steps.append(fn)\n",
    "        return self\n",
    "    def run(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        for fn in self.steps:\n",
    "            df = fn(df)\n",
    "        return df\n",
    "\n",
    "pipe = Pipeline().add(clean_data)\n",
    "processed_df = pipe.run(raw_df)\n",
    "print('Processed shape:', processed_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e04acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Unit Test Cells (pytest Integration)\n",
    "import pytest\n",
    "\n",
    "def test_clean_data_removes_none():\n",
    "    df = pd.DataFrame({'a': [1, None, 3]})\n",
    "    out = clean_data(df)\n",
    "    assert out.shape[0] == 2\n",
    "\n",
    "def test_pipeline_runs():\n",
    "    df = pd.DataFrame({'x': [1,2]})\n",
    "    p = Pipeline().add(lambda d: d.assign(y=d['x']*2))\n",
    "    out = p.run(df)\n",
    "    assert 'y' in out.columns and out['y'].tolist() == [2,4]\n",
    "\n",
    "print('Running inline tests...')\n",
    "pytest.main(['-q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611cad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Progress Journal Append Mechanism\n",
    "JOURNAL_PATH = Path('reports/journal.md')\n",
    "JOURNAL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def append_journal(entry: str):\n",
    "    with open(JOURNAL_PATH, 'a') as f:\n",
    "        f.write(f\"\\n### {timestamp()}\\n{entry}\\n\")\n",
    "    print('Appended journal entry.')\n",
    "\n",
    "append_journal('Initialized core pipeline and utilities.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4d70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Experiment Parameters & Results Recorder\n",
    "import csv\n",
    "EXP_PATH = Path('artifacts/experiments.csv')\n",
    "if not EXP_PATH.exists():\n",
    "    with open(EXP_PATH, 'w', newline='') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(['id','params_json','metric_primary','metric_secondary','timestamp'])\n",
    "\n",
    "next_id = sum(1 for _ in open(EXP_PATH)) - 1\n",
    "params = {'model':'stub','lr':1e-3}\n",
    "metric_primary = 0.0\n",
    "metric_secondary = 0.0\n",
    "with open(EXP_PATH, 'a', newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([next_id, json.dumps(params), metric_primary, metric_secondary, timestamp()])\n",
    "print('Recorded experiment id', next_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Visualization Theme & Helper Registry\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "def plot_distribution(df: pd.DataFrame, col: str):\n",
    "    fig, ax = plt.subplots(figsize=(4,3))\n",
    "    sns.histplot(df[col], ax=ax, kde=True)\n",
    "    fig_path = Path('artifacts/figures') / f'dist_{col}.png'\n",
    "    fig_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(fig_path)\n",
    "    plt.close(fig)\n",
    "    print('Saved figure', fig_path)\n",
    "\n",
    "plot_distribution(processed_df, processed_df.columns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ed13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Artifact Serialization & Hashing\n",
    "import hashlib\n",
    "PARQUET_PATH = Path('artifacts/processed_df.parquet')\n",
    "processed_df.to_parquet(PARQUET_PATH, index=False)\n",
    "sha = hashlib.sha256(PARQUET_PATH.read_bytes()).hexdigest()\n",
    "with open(str(PARQUET_PATH)+'.hash','w') as f:\n",
    "    f.write(sha)\n",
    "print('Saved', PARQUET_PATH, 'SHA256:', sha[:16], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Performance Benchmark Cell\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "_ = processed_df.describe()\n",
    "end = time.perf_counter()\n",
    "bench = {'operation':'describe','duration_secs': end-start, 'timestamp': timestamp()}\n",
    "BENCH_PATH = Path('artifacts/benchmarks.json')\n",
    "prev = []\n",
    "if BENCH_PATH.exists():\n",
    "    prev = json.loads(BENCH_PATH.read_text())\n",
    "prev.append(bench)\n",
    "BENCH_PATH.write_text(json.dumps(prev, indent=2))\n",
    "print('Benchmark recorded:', bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a12849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Error Handling & Retry Decorators\n",
    "import functools, time\n",
    "\n",
    "def retry(exceptions, tries=3, delay=0.5):\n",
    "    def decorator(fn):\n",
    "        @functools.wraps(fn)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            attempt = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    return fn(*args, **kwargs)\n",
    "                except exceptions as e:\n",
    "                    attempt += 1\n",
    "                    if attempt >= tries:\n",
    "                        raise\n",
    "                    time.sleep(delay)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@retry((RuntimeError,), tries=2, delay=0.1)\n",
    "def flaky_load():\n",
    "    import random\n",
    "    if random.random() < 0.5:\n",
    "        raise RuntimeError('Transient read failure')\n",
    "    return {'status':'ok'}\n",
    "\n",
    "print('Flaky load result:', flaky_load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. Notebook to Script Export Automation\n",
    "try:\n",
    "    import nbconvert\n",
    "    NOTEBOOK_PATH = Path('notebooks/build_tracking.ipynb')\n",
    "    SCRIPT_OUT = Path('scripts/project_pipeline.py')\n",
    "    SCRIPT_OUT.parent.mkdir(exist_ok=True)\n",
    "    # Use nbconvert API\n",
    "    exporter = nbconvert.PythonExporter()\n",
    "    script_body, _ = exporter.from_file(str(NOTEBOOK_PATH))\n",
    "    SCRIPT_OUT.write_text(script_body)\n",
    "    print('Exported notebook to', SCRIPT_OUT)\n",
    "except Exception as e:\n",
    "    print('nbconvert export failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adfc592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. Final State Commit Helper (Git Integration)\n",
    "import subprocess\n",
    "try:\n",
    "    subprocess.run(['git','add','.'], check=True)\n",
    "    subprocess.run(['git','commit','-m','auto: tracking snapshot update'], check=True)\n",
    "    print('Auto commit created.')\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print('Git commit skipped or failed:', e)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
