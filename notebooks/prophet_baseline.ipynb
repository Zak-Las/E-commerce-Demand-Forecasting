{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8113cf",
   "metadata": {},
   "source": [
    "# Prophet Baseline Notebook (Lean Version)\n",
    "\n",
    "Purpose: Establish a transparent statistical baseline for 30-day item-level demand forecasting on the M5 subset.\n",
    "Scope: CPU-only, small item subset (<=10) + aggregate series; hold-out last 30 days for validation.\n",
    "Artifacts: metrics JSON saved to artifacts/models/prophet_metrics.json for README results table.\n",
    "\n",
    "Steps:\n",
    "1. Load processed panel (or synthetic fallback identical to data_prep).\n",
    "2. Train/validation split (last FORECAST_LENGTH days).\n",
    "3. Fit Prophet on aggregate series -> forecast -> compute WAPE/MAE.\n",
    "4. Fit Prophet per item (first N_ITEMS) -> hold-out validation metrics.\n",
    "5. Save aggregated metrics JSON.\n",
    "\n",
    "Metric definitions: WAPE (weighted abs pct error), Accuracy = 100 - WAPE.\n",
    "\n",
    "Run order: Execute all cells top-to-bottom; then fill README results placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbb88a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path augmented with: []\n",
      "Import test: src package available.\n"
     ]
    }
   ],
   "source": [
    "# Path injection to ensure 'src' package resolvable when notebook launched from notebooks/\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "_nb_root = Path.cwd()\n",
    "# If current working dir ends with 'notebooks', ascend one level for repo root\n",
    "_repo_root = _nb_root if (_nb_root / 'src').exists() else _nb_root.parent\n",
    "_src = _repo_root / 'src'\n",
    "\n",
    "if _src.exists():\n",
    "    paths_added = []\n",
    "    for p in (str(_repo_root), str(_src)):\n",
    "        if p not in sys.path:\n",
    "            sys.path.insert(0, p)\n",
    "            paths_added.append(p)\n",
    "    print('sys.path augmented with:', paths_added)\n",
    "else:\n",
    "    print('Warning: src directory not found at expected path:', _src)\n",
    "    print('CWD:', _nb_root)\n",
    "    print('Directory listing of parent:', [d for d in _nb_root.parent.iterdir()][:15])\n",
    "\n",
    "try:\n",
    "    import src\n",
    "    print('Import test: src package available.')\n",
    "except ModuleNotFoundError:\n",
    "    print('ModuleNotFoundError persists. Head of sys.path:', sys.path[:8])\n",
    "    # Provide troubleshooting hints\n",
    "    print('Troubleshooting: Ensure you started Jupyter from repo root or run \"cd ..\" before opening notebook.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4e72cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & constants\n",
    "import pandas as pd, numpy as np, json, math\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "except ImportError:\n",
    "    print('Prophet not installed. Install with `pip install prophet` and rerun.')\n",
    "    Prophet = None\n",
    "from src.evaluation.metrics import wape, mae\n",
    "PANEL_PATH = Path('data/processed/m5_panel_subset.parquet')\n",
    "ARTIFACT_DIR = Path('artifacts/models')\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FORECAST_LENGTH = 30\n",
    "INPUT_LENGTH = 28 * 4  # aligns with N-BEATS history\n",
    "N_ITEMS = 10  # limit for lean baseline\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60000fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panel parquet missing -> generating synthetic fallback (20 items x 200 days).\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'data/processed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m             rows\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m: item, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m: d, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdemand\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(val)})\n\u001b[1;32m     17\u001b[0m     panel_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mpanel_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPANEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     source_note \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSynthetic fallback saved.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m panel_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(panel_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/DataCamp/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DataCamp/lib/python3.11/site-packages/pandas/core/frame.py:3118\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   3039\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3114\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   3115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 3118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DataCamp/lib/python3.11/site-packages/pandas/io/parquet.py:482\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    480\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 482\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/anaconda3/envs/DataCamp/lib/python3.11/site-packages/pandas/io/parquet.py:199\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m     merged_metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexisting_metadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdf_metadata}\n\u001b[1;32m    197\u001b[0m     table \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mreplace_schema_metadata(merged_metadata)\n\u001b[0;32m--> 199\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(path_or_handle, io\u001b[38;5;241m.\u001b[39mBufferedWriter)\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(path_or_handle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle\u001b[38;5;241m.\u001b[39mname, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m))\n\u001b[1;32m    210\u001b[0m ):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mbytes\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/DataCamp/lib/python3.11/site-packages/pandas/io/parquet.py:141\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    131\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/DataCamp/lib/python3.11/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DataCamp/lib/python3.11/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'data/processed'"
     ]
    }
   ],
   "source": [
    "# Load panel or synthetic fallback\n",
    "if PANEL_PATH.exists():\n",
    "    panel_df = pd.read_parquet(PANEL_PATH)\n",
    "    source_note = f'Loaded panel: {PANEL_PATH}'\n",
    "else:\n",
    "    print('Panel parquet missing -> generating synthetic fallback (20 items x 200 days).')\n",
    "    items = [f'ITEM_{i:03d}' for i in range(20)]\n",
    "    dates = pd.date_range('2024-01-01', periods=200, freq='D')\n",
    "    rows = []\n",
    "    for item in items:\n",
    "        base = np.random.randint(5, 25)\n",
    "        seasonal = np.sin(np.linspace(0, 12 * math.pi, len(dates))) * np.random.uniform(3, 8)\n",
    "        noise = np.random.randn(len(dates)) * np.random.uniform(0.5, 2.0)\n",
    "        demand = (base + seasonal + noise).clip(min=0).round(2)\n",
    "        for d, val in zip(dates, demand):\n",
    "            rows.append({'item_id': item, 'date': d, 'demand': float(val)})\n",
    "    panel_df = pd.DataFrame(rows)\n",
    "    # Ensure directory exists before saving\n",
    "    PANEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    panel_df.to_parquet(PANEL_PATH, index=False)\n",
    "    source_note = 'Synthetic fallback saved.'\n",
    "panel_df['date'] = pd.to_datetime(panel_df['date'])\n",
    "print(source_note)\n",
    "print(panel_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8126dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split (hold out last FORECAST_LENGTH days)\n",
    "unique_dates = sorted(panel_df['date'].unique())\n",
    "assert len(unique_dates) > FORECAST_LENGTH, 'Not enough days for hold-out validation.'\n",
    "cutoff_date = unique_dates[-FORECAST_LENGTH]  # first day of validation segment\n",
    "train_df = panel_df[panel_df['date'] < cutoff_date]\n",
    "valid_df = panel_df[panel_df['date'] >= cutoff_date]\n",
    "print('Cutoff date:', cutoff_date)\n",
    "print('Train range:', train_df['date'].min().date(), '->', train_df['date'].max().date(), '| rows:', len(train_df))\n",
    "print('Valid range:', valid_df['date'].min().date(), '->', valid_df['date'].max().date(), '| rows:', len(valid_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e6a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate series modeling with Prophet\n",
    "if Prophet is None:\n",
    "    print('Skip aggregate model: Prophet not installed.')\n",
    "else:\n",
    "    agg_train = train_df.groupby('date', as_index=False)['demand'].sum().rename(columns={'date':'ds','demand':'y'})\n",
    "    agg_valid = valid_df.groupby('date', as_index=False)['demand'].sum().rename(columns={'date':'ds','demand':'y'})\n",
    "    model_agg = Prophet()\n",
    "    model_agg.fit(agg_train)\n",
    "    future_agg = model_agg.make_future_dataframe(periods=FORECAST_LENGTH)\n",
    "    fc_agg = model_agg.predict(future_agg)\n",
    "    fc_valid = fc_agg.tail(FORECAST_LENGTH)\n",
    "    # Align with actual validation dates\n",
    "    actual_valid = agg_valid['y'].values\n",
    "    pred_valid = fc_valid['yhat'].values\n",
    "    agg_wape = wape(actual_valid, pred_valid)\n",
    "    agg_mae = mae(actual_valid, pred_valid)\n",
    "    agg_accuracy = 100 - agg_wape if not math.isnan(agg_wape) else float('nan')\n",
    "    print(f'Agg validation WAPE: {agg_wape:.2f} | MAE: {agg_mae:.2f} | Accuracy: {agg_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-item modeling loop (subset)\n",
    "item_ids = sorted(train_df['item_id'].unique())[:N_ITEMS]\n",
    "item_metric_rows = []\n",
    "if Prophet is None:\n",
    "    print('Skip per-item loop: Prophet not installed.')\n",
    "else:\n",
    "    for itm in item_ids:\n",
    "        sub_train = train_df[train_df.item_id == itm].sort_values('date')\n",
    "        sub_valid = valid_df[valid_df.item_id == itm].sort_values('date')\n",
    "        # Require minimum history\n",
    "        if len(sub_train) < INPUT_LENGTH:\n",
    "            continue\n",
    "        df_train = sub_train[['date','demand']].rename(columns={'date':'ds','demand':'y'})\n",
    "        df_valid = sub_valid[['date','demand']].rename(columns={'date':'ds','demand':'y'})\n",
    "        m_item = Prophet()\n",
    "        m_item.fit(df_train)\n",
    "        future_item = m_item.make_future_dataframe(periods=FORECAST_LENGTH)\n",
    "        fc_item = m_item.predict(future_item).tail(FORECAST_LENGTH)\n",
    "        y_true = df_valid['y'].values\n",
    "        y_pred = fc_item['yhat'].values\n",
    "        w = wape(y_true, y_pred)\n",
    "        a = mae(y_true, y_pred)\n",
    "        item_metric_rows.append({'item_id': itm, 'wape': w, 'mae': a})\n",
    "    per_item_df = pd.DataFrame(item_metric_rows)\n",
    "    if not per_item_df.empty:\n",
    "        mean_wape = per_item_df['wape'].mean()\n",
    "        mean_mae = per_item_df['mae'].mean()\n",
    "        print(f'Per-item mean WAPE (subset): {mean_wape:.2f} | MAE: {mean_mae:.2f}')\n",
    "    else:\n",
    "        mean_wape = float('nan'); mean_mae = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e619f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics artifact\n",
    "metrics = {\n",
    "    'source': source_note,\n",
    "    'forecast_length': FORECAST_LENGTH,\n",
    "    'input_length': INPUT_LENGTH,\n",
    "    'aggregate_wape': agg_wape if 'agg_wape' in globals() else None,\n",
    "    'aggregate_mae': agg_mae if 'agg_mae' in globals() else None,\n",
    "    'aggregate_accuracy': agg_accuracy if 'agg_accuracy' in globals() else None,\n",
    "    'per_item_mean_wape': mean_wape if 'mean_wape' in globals() else None,\n",
    "    'per_item_mean_mae': mean_mae if 'mean_mae' in globals() else None,\n",
    "    'n_items_modeled': len(item_metric_rows) if 'item_metric_rows' in globals() else 0,\n",
    "    'seed': SEED,\n",
    "}\n",
    "out_path = ARTIFACT_DIR / 'prophet_metrics.json'\n",
    "with open(out_path,'w') as f: json.dump(metrics, f, indent=2)\n",
    "print('Saved Prophet metrics ->', out_path)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook summary markdown (for human-readable insight)\n",
    "from pathlib import Path\n",
    "md_lines = [\n",
    "    '# Prophet Baseline Summary',\n",
    "    f'Aggregate WAPE: {metrics.get(\"aggregate_wape\")}',\n",
    "    f'Aggregate MAE: {metrics.get(\"aggregate_mae\")}',\n",
    "    f'Per-item Mean WAPE: {metrics.get(\"per_item_mean_wape\")}',\n",
    "    f'Items Modeled: {metrics.get(\"n_items_modeled\")}',\n",
    "    f'Forecast Length: {FORECAST_LENGTH}',\n",
    "    '',\n",
    "    'Use these values to populate the README results table.'\n",
    "]\n",
    "summary_path = ARTIFACT_DIR / 'prophet_metrics_summary.md'\n",
    "with open(summary_path,'w') as f: f.write('\\n'.join(md_lines))\n",
    "print('Wrote summary markdown ->', summary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next steps note (markdown)\n",
    "print('Next: Run nbeats_training.ipynb to compare metrics and generate model card.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataCamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
