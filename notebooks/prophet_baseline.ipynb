{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8113cf",
   "metadata": {},
   "source": [
    "# Prophet Baseline Notebook (Lean Version)\n",
    "\n",
    "Purpose: Establish a transparent statistical baseline for 30-day item-level demand forecasting on the M5 subset.\n",
    "Scope: CPU-only, small item subset (<=10) + aggregate series; hold-out last 30 days for validation.\n",
    "Artifacts: metrics JSON saved to artifacts/models/prophet_metrics.json for README results table.\n",
    "\n",
    "Steps:\n",
    "1. Load processed panel (or synthetic fallback identical to data_prep).\n",
    "2. Train/validation split (last FORECAST_LENGTH days).\n",
    "3. Fit Prophet on aggregate series -> forecast -> compute WAPE/MAE.\n",
    "4. Fit Prophet per item (first N_ITEMS) -> hold-out validation metrics.\n",
    "5. Save aggregated metrics JSON.\n",
    "\n",
    "Metric definitions: WAPE (weighted abs pct error), Accuracy = 100 - WAPE.\n",
    "\n",
    "Run order: Execute all cells top-to-bottom; then fill README results placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbb88a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path augmented with: []\n",
      "Import test: src package available.\n"
     ]
    }
   ],
   "source": [
    "# Path injection to ensure 'src' package resolvable when notebook launched from notebooks/\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "_nb_root = Path.cwd()\n",
    "# If current working dir ends with 'notebooks', ascend one level for repo root\n",
    "_repo_root = _nb_root if (_nb_root / 'src').exists() else _nb_root.parent\n",
    "_src = _repo_root / 'src'\n",
    "\n",
    "if _src.exists():\n",
    "    paths_added = []\n",
    "    for p in (str(_repo_root), str(_src)):\n",
    "        if p not in sys.path:\n",
    "            sys.path.insert(0, p)\n",
    "            paths_added.append(p)\n",
    "    print('sys.path augmented with:', paths_added)\n",
    "else:\n",
    "    print('Warning: src directory not found at expected path:', _src)\n",
    "    print('CWD:', _nb_root)\n",
    "    print('Directory listing of parent:', [d for d in _nb_root.parent.iterdir()][:15])\n",
    "\n",
    "try:\n",
    "    import src\n",
    "    print('Import test: src package available.')\n",
    "except ModuleNotFoundError:\n",
    "    print('ModuleNotFoundError persists. Head of sys.path:', sys.path[:8])\n",
    "    # Provide troubleshooting hints\n",
    "    print('Troubleshooting: Ensure you started Jupyter from repo root or run \"cd ..\" before opening notebook.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4e72cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & constants\n",
    "import pandas as pd, numpy as np, json, math\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "except ImportError:\n",
    "    print('Prophet not installed. Install with `pip install prophet` and rerun.')\n",
    "    Prophet = None\n",
    "from src.evaluation.metrics import wape, mae\n",
    "PANEL_PATH = Path('data/processed/m5_panel_subset.parquet')\n",
    "ARTIFACT_DIR = Path('artifacts/models')\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FORECAST_LENGTH = 30\n",
    "INPUT_LENGTH = 28 * 4  # aligns with N-BEATS history\n",
    "N_ITEMS = 10  # limit for lean baseline\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60000fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panel parquet missing -> generating synthetic fallback (20 items x 200 days).\n",
      "Synthetic fallback saved.\n",
      "    item_id       date  demand\n",
      "0  ITEM_000 2024-01-01   16.10\n",
      "1  ITEM_000 2024-01-02   17.33\n",
      "2  ITEM_000 2024-01-03   16.59\n",
      "3  ITEM_000 2024-01-04   18.31\n",
      "4  ITEM_000 2024-01-05   19.48\n"
     ]
    }
   ],
   "source": [
    "# Load panel or synthetic fallback\n",
    "if PANEL_PATH.exists():\n",
    "    panel_df = pd.read_parquet(PANEL_PATH)\n",
    "    source_note = f'Loaded panel: {PANEL_PATH}'\n",
    "else:\n",
    "    print('Panel parquet missing -> generating synthetic fallback (20 items x 200 days).')\n",
    "    items = [f'ITEM_{i:03d}' for i in range(20)]\n",
    "    dates = pd.date_range('2024-01-01', periods=200, freq='D')\n",
    "    rows = []\n",
    "    for item in items:\n",
    "        base = np.random.randint(5, 25)\n",
    "        seasonal = np.sin(np.linspace(0, 12 * math.pi, len(dates))) * np.random.uniform(3, 8)\n",
    "        noise = np.random.randn(len(dates)) * np.random.uniform(0.5, 2.0)\n",
    "        demand = (base + seasonal + noise).clip(min=0).round(2)\n",
    "        for d, val in zip(dates, demand):\n",
    "            rows.append({'item_id': item, 'date': d, 'demand': float(val)})\n",
    "    panel_df = pd.DataFrame(rows)\n",
    "    # Ensure directory exists before saving\n",
    "    PANEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    panel_df.to_parquet(PANEL_PATH, index=False)\n",
    "    source_note = 'Synthetic fallback saved.'\n",
    "panel_df['date'] = pd.to_datetime(panel_df['date'])\n",
    "print(source_note)\n",
    "print(panel_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8126dd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff date: 2024-06-19 00:00:00\n",
      "Train range: 2024-01-01 -> 2024-06-18 | rows: 3400\n",
      "Valid range: 2024-06-19 -> 2024-07-18 | rows: 600\n"
     ]
    }
   ],
   "source": [
    "# Train/validation split (hold out last FORECAST_LENGTH days)\n",
    "unique_dates = sorted(panel_df['date'].unique())\n",
    "assert len(unique_dates) > FORECAST_LENGTH, 'Not enough days for hold-out validation.'\n",
    "cutoff_date = unique_dates[-FORECAST_LENGTH]  # first day of validation segment\n",
    "train_df = panel_df[panel_df['date'] < cutoff_date]\n",
    "valid_df = panel_df[panel_df['date'] >= cutoff_date]\n",
    "print('Cutoff date:', cutoff_date)\n",
    "print('Train range:', train_df['date'].min().date(), '->', train_df['date'].max().date(), '| rows:', len(train_df))\n",
    "print('Valid range:', valid_df['date'].min().date(), '->', valid_df['date'].max().date(), '| rows:', len(valid_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd9e6a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:15:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agg validation WAPE: 24.81 | MAE: 77.09 | Accuracy: 75.19\n"
     ]
    }
   ],
   "source": [
    "# Aggregate series modeling with Prophet\n",
    "if Prophet is None:\n",
    "    print('Skip aggregate model: Prophet not installed.')\n",
    "else:\n",
    "    agg_train = train_df.groupby('date', as_index=False)['demand'].sum().rename(columns={'date':'ds','demand':'y'})\n",
    "    agg_valid = valid_df.groupby('date', as_index=False)['demand'].sum().rename(columns={'date':'ds','demand':'y'})\n",
    "    model_agg = Prophet()\n",
    "    model_agg.fit(agg_train)\n",
    "    future_agg = model_agg.make_future_dataframe(periods=FORECAST_LENGTH)\n",
    "    fc_agg = model_agg.predict(future_agg)\n",
    "    fc_valid = fc_agg.tail(FORECAST_LENGTH)\n",
    "    # Align with actual validation dates\n",
    "    actual_valid = agg_valid['y'].values\n",
    "    pred_valid = fc_valid['yhat'].values\n",
    "    agg_wape = wape(actual_valid, pred_valid)\n",
    "    agg_mae = mae(actual_valid, pred_valid)\n",
    "    agg_accuracy = 100 - agg_wape if not math.isnan(agg_wape) else float('nan')\n",
    "    print(f'Agg validation WAPE: {agg_wape:.2f} | MAE: {agg_mae:.2f} | Accuracy: {agg_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b72542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:15:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:55 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-item mean WAPE (subset): 38.78 | MAE: 3.94\n"
     ]
    }
   ],
   "source": [
    "# Per-item modeling loop (subset)\n",
    "item_ids = sorted(train_df['item_id'].unique())[:N_ITEMS]\n",
    "item_metric_rows = []\n",
    "if Prophet is None:\n",
    "    print('Skip per-item loop: Prophet not installed.')\n",
    "else:\n",
    "    for itm in item_ids:\n",
    "        sub_train = train_df[train_df.item_id == itm].sort_values('date')\n",
    "        sub_valid = valid_df[valid_df.item_id == itm].sort_values('date')\n",
    "        # Require minimum history\n",
    "        if len(sub_train) < INPUT_LENGTH:\n",
    "            continue\n",
    "        df_train = sub_train[['date','demand']].rename(columns={'date':'ds','demand':'y'})\n",
    "        df_valid = sub_valid[['date','demand']].rename(columns={'date':'ds','demand':'y'})\n",
    "        m_item = Prophet()\n",
    "        m_item.fit(df_train)\n",
    "        future_item = m_item.make_future_dataframe(periods=FORECAST_LENGTH)\n",
    "        fc_item = m_item.predict(future_item).tail(FORECAST_LENGTH)\n",
    "        y_true = df_valid['y'].values\n",
    "        y_pred = fc_item['yhat'].values\n",
    "        w = wape(y_true, y_pred)\n",
    "        a = mae(y_true, y_pred)\n",
    "        item_metric_rows.append({'item_id': itm, 'wape': w, 'mae': a})\n",
    "    per_item_df = pd.DataFrame(item_metric_rows)\n",
    "    if not per_item_df.empty:\n",
    "        mean_wape = per_item_df['wape'].mean()\n",
    "        mean_mae = per_item_df['mae'].mean()\n",
    "        print(f'Per-item mean WAPE (subset): {mean_wape:.2f} | MAE: {mean_mae:.2f}')\n",
    "    else:\n",
    "        mean_wape = float('nan'); mean_mae = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00e619f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Prophet metrics -> artifacts/models/prophet_metrics.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source': 'Synthetic fallback saved.',\n",
       " 'forecast_length': 30,\n",
       " 'input_length': 112,\n",
       " 'aggregate_wape': 24.805205452672528,\n",
       " 'aggregate_mae': 77.0892040857248,\n",
       " 'aggregate_accuracy': 75.19479454732748,\n",
       " 'per_item_mean_wape': 38.78456596784034,\n",
       " 'per_item_mean_mae': 3.941935927562079,\n",
       " 'n_items_modeled': 10,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save metrics artifact\n",
    "metrics = {\n",
    "    'source': source_note,\n",
    "    'forecast_length': FORECAST_LENGTH,\n",
    "    'input_length': INPUT_LENGTH,\n",
    "    'aggregate_wape': agg_wape if 'agg_wape' in globals() else None,\n",
    "    'aggregate_mae': agg_mae if 'agg_mae' in globals() else None,\n",
    "    'aggregate_accuracy': agg_accuracy if 'agg_accuracy' in globals() else None,\n",
    "    'per_item_mean_wape': mean_wape if 'mean_wape' in globals() else None,\n",
    "    'per_item_mean_mae': mean_mae if 'mean_mae' in globals() else None,\n",
    "    'n_items_modeled': len(item_metric_rows) if 'item_metric_rows' in globals() else 0,\n",
    "    'seed': SEED,\n",
    "}\n",
    "out_path = ARTIFACT_DIR / 'prophet_metrics.json'\n",
    "with open(out_path,'w') as f: json.dump(metrics, f, indent=2)\n",
    "print('Saved Prophet metrics ->', out_path)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69ab34db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote summary markdown -> artifacts/models/prophet_metrics_summary.md\n"
     ]
    }
   ],
   "source": [
    "# Notebook summary markdown (for human-readable insight)\n",
    "from pathlib import Path\n",
    "md_lines = [\n",
    "    '# Prophet Baseline Summary',\n",
    "    f'Aggregate WAPE: {metrics.get(\"aggregate_wape\")}',\n",
    "    f'Aggregate MAE: {metrics.get(\"aggregate_mae\")}',\n",
    "    f'Per-item Mean WAPE: {metrics.get(\"per_item_mean_wape\")}',\n",
    "    f'Items Modeled: {metrics.get(\"n_items_modeled\")}',\n",
    "    f'Forecast Length: {FORECAST_LENGTH}',\n",
    "    '',\n",
    "    'Use these values to populate the README results table.'\n",
    "]\n",
    "summary_path = ARTIFACT_DIR / 'prophet_metrics_summary.md'\n",
    "with open(summary_path,'w') as f: f.write('\\n'.join(md_lines))\n",
    "print('Wrote summary markdown ->', summary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0eb38b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next: Run nbeats_training.ipynb to compare metrics and generate model card.\n"
     ]
    }
   ],
   "source": [
    "# Next steps note (markdown)\n",
    "print('Next: Run nbeats_training.ipynb to compare metrics and generate model card.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataCamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
