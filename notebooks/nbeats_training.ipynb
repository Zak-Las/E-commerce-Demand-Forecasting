{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74580b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using enforced CPU device: cpu | accelerator=cpu\n"
     ]
    }
   ],
   "source": [
    "# Environment & device setup (CPU-only enforced)\n",
    "import torch, os\n",
    "os.environ['TORCH_DISABLE_TORCHDYNAMO'] = '1'  # ensure no dynamo/inductor surprises\n",
    "os.environ.pop('PYTORCH_ENABLE_MPS_FALLBACK', None)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "accelerator = 'cpu'\n",
    "print(f'Using enforced CPU device: {device} | accelerator={accelerator}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d98512b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path augmented with: ['/Users/zak/Repos/E-commerce-Demand-Forecasting', '/Users/zak/Repos/E-commerce-Demand-Forecasting/src']\n",
      "Import test: src package available.\n"
     ]
    }
   ],
   "source": [
    "# Path injection to ensure 'src' package resolvable when notebook launched from notebooks/\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "_nb_root = Path.cwd()\n",
    "# If current working dir ends with 'notebooks', ascend one level for repo root\n",
    "_repo_root = _nb_root if (_nb_root / 'src').exists() else _nb_root.parent\n",
    "_src = _repo_root / 'src'\n",
    "\n",
    "if _src.exists():\n",
    "    paths_added = []\n",
    "    for p in (str(_repo_root), str(_src)):\n",
    "        if p not in sys.path:\n",
    "            sys.path.insert(0, p)\n",
    "            paths_added.append(p)\n",
    "    print('sys.path augmented with:', paths_added)\n",
    "else:\n",
    "    print('Warning: src directory not found at expected path:', _src)\n",
    "    print('CWD:', _nb_root)\n",
    "    print('Directory listing of parent:', [d for d in _nb_root.parent.iterdir()][:15])\n",
    "\n",
    "try:\n",
    "    import src\n",
    "    print('Import test: src package available.')\n",
    "except ModuleNotFoundError:\n",
    "    print('ModuleNotFoundError persists. Head of sys.path:', sys.path[:8])\n",
    "    # Provide troubleshooting hints\n",
    "    print('Troubleshooting: Ensure you started Jupyter from repo root or run \"cd ..\" before opening notebook.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f227672",
   "metadata": {},
   "source": [
    "# N-BEATS Training Notebook\n",
    "\n",
    "Train the minimal N-BEATS implementation on the panel parquet subset.\n",
    "\n",
    "## Objectives\n",
    "1. Load processed panel data (item_id, date, demand).\n",
    "2. Create sliding window dataset (input_length -> forecast_length).\n",
    "3. Train N-BEATS LightningModule for a few epochs.\n",
    "4. Compute validation metrics (MAE, WAPE).\n",
    "5. Save checkpoint + metrics artifacts.\n",
    "\n",
    "If the panel file is missing, a synthetic dataset will be generated so the pipeline can run end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "338f002f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0\n",
      "Lightning version: 2.5.5\n",
      "Device: cpu\n",
      "Accelerator: cpu\n",
      "Backend note: Preserving pre-set CPU device\n"
     ]
    }
   ],
   "source": [
    "# Imports (retain existing CPU device if already defined)\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from src.models.nbeats_module import NBeatsModule, NBeatsConfig\n",
    "from src.data.dataset_nbeats import PanelForecastDataset, PanelWindowConfig, split_dataset\n",
    "\n",
    "# Only set device if not previously forced to CPU\n",
    "if 'device' not in globals() or device.type != 'cpu':\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device('mps')\n",
    "        accelerator = 'mps'\n",
    "        backend_note = 'Using Apple Silicon MPS backend'\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        accelerator = 'gpu'\n",
    "        backend_note = f'Using CUDA GPU: {torch.cuda.get_device_name(0)}'\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        accelerator = 'cpu'\n",
    "        backend_note = 'Falling back to CPU'\n",
    "else:\n",
    "    backend_note = 'Preserving pre-set CPU device'\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('Lightning version:', pl.__version__)\n",
    "print('Device:', device)\n",
    "print('Accelerator:', accelerator)\n",
    "print('Backend note:', backend_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4eb01be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config -> epochs: 20 | batch_size: 64 | stacks: 3 blocks/stack: 3 layer_width: 768\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters (CPU training - 20 epochs)\n",
    "PANEL_PATH = Path('data/processed/m5_panel_subset.parquet')\n",
    "ARTIFACTS_DIR = Path('artifacts/models')\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INPUT_LENGTH = 28 * 4  # 112 days lookback\n",
    "FORECAST_LENGTH = 30\n",
    "BATCH_SIZE = 64  # CPU-friendly batch size\n",
    "EPOCHS = 20     # Requested run length\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_STACKS = 3\n",
    "BLOCKS_PER_STACK = 3\n",
    "LAYER_WIDTH = 768\n",
    "N_LAYERS = 4\n",
    "DROPOUT = 0.05\n",
    "MAX_ITEMS = 50\n",
    "MAX_WINDOWS_PER_ITEM = 40\n",
    "VAL_FRACTION = 0.1\n",
    "SEED = 42\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "print('Config -> epochs:', EPOCHS, '| batch_size:', BATCH_SIZE, '| stacks:', NUM_STACKS, 'blocks/stack:', BLOCKS_PER_STACK, 'layer_width:', LAYER_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9ce519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading panel from data/processed/m5_panel_subset.parquet\n",
      "    item_id       date  demand\n",
      "0  ITEM_000 2024-01-01   11.90\n",
      "1  ITEM_000 2024-01-02   11.30\n",
      "2  ITEM_000 2024-01-03   11.60\n",
      "3  ITEM_000 2024-01-04   18.30\n",
      "4  ITEM_000 2024-01-05   15.64\n",
      "Panel shape: (4000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load or create synthetic panel\n",
    "if PANEL_PATH.exists():\n",
    "    print('Loading panel from', PANEL_PATH)\n",
    "    panel_df = pd.read_parquet(PANEL_PATH)\n",
    "else:\n",
    "    print('Panel not found. Creating synthetic panel for demo...')\n",
    "    # Synthetic: 20 items, 200 days, simple seasonal pattern + noise\n",
    "    import numpy as np\n",
    "    items = [f'ITEM_{i:03d}' for i in range(20)]\n",
    "    dates = pd.date_range('2024-01-01', periods=200, freq='D')\n",
    "    rows = []\n",
    "    for item in items:\n",
    "        base = np.random.randint(5, 25)\n",
    "        seasonal = np.sin(np.linspace(0, 12 * math.pi, len(dates))) * np.random.uniform(3, 8)\n",
    "        noise = np.random.randn(len(dates)) * np.random.uniform(0.5, 2.0)\n",
    "        demand = (base + seasonal + noise).clip(min=0).round(2)\n",
    "        for d, val in zip(dates, demand):\n",
    "            rows.append({'item_id': item, 'date': d, 'demand': float(val)})\n",
    "    panel_df = pd.DataFrame(rows)\n",
    "    PANEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    panel_df.to_parquet(PANEL_PATH, index=False)\n",
    "print(panel_df.head())\n",
    "print('Panel shape:', panel_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905be5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff date for validation segment: 2024-06-19 00:00:00\n",
      "Train date range: 2024-01-01 00:00:00 -> 2024-06-18 00:00:00 | rows: 3400\n",
      "Val+history date range: 2024-01-30 00:00:00 -> 2024-07-18 00:00:00 | rows: 3420\n",
      "Windows -> train: 580 val: 600\n",
      "Sample shapes -> x: torch.Size([112]) y: torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "# Build dataset windows with chronological split\n",
    "# Determine cutoff date for validation based on VAL_FRACTION of unique days\n",
    "unique_dates = sorted(panel_df['date'].unique())\n",
    "val_days = max(FORECAST_LENGTH, int(len(unique_dates) * VAL_FRACTION))\n",
    "# Ensure we have enough history for validation windows\n",
    "val_history_needed = INPUT_LENGTH + FORECAST_LENGTH\n",
    "cutoff_index = len(unique_dates) - val_days\n",
    "cutoff_date = unique_dates[cutoff_index]\n",
    "\n",
    "# Train: all dates strictly before cutoff_date\n",
    "train_df = panel_df[panel_df['date'] < cutoff_date]\n",
    "# Validation: last segment plus required preceding history window\n",
    "val_start_history_date = unique_dates[max(0, cutoff_index - (val_history_needed - 1))]\n",
    "val_df = panel_df[panel_df['date'] >= val_start_history_date]\n",
    "\n",
    "print('Cutoff date for validation segment:', cutoff_date)\n",
    "print('Train date range:', train_df['date'].min(), '->', train_df['date'].max(), '| rows:', len(train_df))\n",
    "print('Val+history date range:', val_df['date'].min(), '->', val_df['date'].max(), '| rows:', len(val_df))\n",
    "\n",
    "# Persist temporary parquet shards (avoids modifying original panel file)\n",
    "train_path = PANEL_PATH.parent / 'm5_panel_subset_train.parquet'\n",
    "val_path = PANEL_PATH.parent / 'm5_panel_subset_val.parquet'\n",
    "train_df.to_parquet(train_path, index=False)\n",
    "val_df.to_parquet(val_path, index=False)\n",
    "\n",
    "cfg_ds = PanelWindowConfig(input_length=INPUT_LENGTH, forecast_length=FORECAST_LENGTH, max_items=MAX_ITEMS, max_windows_per_item=MAX_WINDOWS_PER_ITEM)\n",
    "train_ds = PanelForecastDataset(train_path, cfg_ds)\n",
    "val_ds = PanelForecastDataset(val_path, cfg_ds)\n",
    "print('Windows -> train:', len(train_ds), 'val:', len(val_ds))\n",
    "# Inspect one sample\n",
    "x0, y0 = train_ds[0]\n",
    "print('Sample shapes -> x:', x0.shape, 'y:', y0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6de8497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches -> train: 10 val: 10 | workers: 6 | batch_size: 64\n"
     ]
    }
   ],
   "source": [
    "# DataLoaders (batch 512)\n",
    "num_workers = 6  # adjust based on CPU cores\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, persistent_workers=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, num_workers=num_workers, persistent_workers=True)\n",
    "print('Batches -> train:', len(train_loader), 'val:', len(val_loader), '| workers:', num_workers, '| batch_size:', BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbae1d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBeatsModule(\n",
      "  (stacks): ModuleList(\n",
      "    (0-2): 3 x ModuleList(\n",
      "      (0-2): 3 x NBeatsBlock(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=112, out_features=768, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.05, inplace=False)\n",
      "          (3): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (4): ReLU()\n",
      "          (5): Dropout(p=0.05, inplace=False)\n",
      "          (6): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (7): ReLU()\n",
      "          (8): Dropout(p=0.05, inplace=False)\n",
      "          (9): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (10): ReLU()\n",
      "          (11): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (backcast_head): Linear(in_features=768, out_features=112, bias=True)\n",
      "        (forecast_head): Linear(in_features=768, out_features=30, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loss_fn): MSELoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "cfg_model = NBeatsConfig(input_length=INPUT_LENGTH, forecast_length=FORECAST_LENGTH, learning_rate=LEARNING_RATE, num_stacks=NUM_STACKS, num_blocks_per_stack=BLOCKS_PER_STACK, layer_width=LAYER_WIDTH, n_layers=N_LAYERS, dropout=DROPOUT)\n",
    "model = NBeatsModule(cfg_model).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d292d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled successfully with torch.compile (mode=reduce-overhead).\n"
     ]
    }
   ],
   "source": [
    "# Optional experimental torch.compile step (PyTorch 2.x)\n",
    "import torch\n",
    "\n",
    "if hasattr(torch, 'compile'):\n",
    "    compile_mode = 'reduce-overhead'  # alternatives: 'max-autotune', 'default'\n",
    "    try:\n",
    "        # For MPS backend, fullgraph=False tends to be safer; dynamic shapes may break some passes.\n",
    "        model = torch.compile(model, mode=compile_mode, fullgraph=False)\n",
    "        print(f'Model compiled successfully with torch.compile (mode={compile_mode}).')\n",
    "    except Exception as e:\n",
    "        print('torch.compile failed:', type(e).__name__, str(e)[:300])\n",
    "        print('Falling back to original (uncompiled) model.')\n",
    "else:\n",
    "    print('torch.compile not available in this PyTorch build.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8941fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU-only mode: skipping torch.compile. Model remains uncompiled.\n"
     ]
    }
   ],
   "source": [
    "# CPU-only mode: skip torch.compile (no benefit vs risk of overhead here)\n",
    "print('CPU-only mode: skipping torch.compile. Model remains uncompiled.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b23d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Metric Interpretation & Overfitting Guide (updated for scaled run)\n",
    "# Refer to earlier explanations. After scaling model, monitor:\n",
    "#  - train_loss vs val_loss divergence\n",
    "#  - val_wape flattening\n",
    "#  - potential instability if precision < 32 on MPS\n",
    "# If OOM occurs, reduce BATCH_SIZE first, then LAYER_WIDTH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CPU Training (20 epochs) ===\n",
      "Device: cpu | Epochs: 20 | Batch size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zak/anaconda3/envs/DataCamp/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | stacks  | ModuleList | 17.7 M | train\n",
      "1 | loss_fn | MSELoss    | 0      | train\n",
      "-----------------------------------------------\n",
      "17.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.7 M    Total params\n",
      "70.839    Total estimated model params size (MB)\n",
      "149       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | stacks  | ModuleList | 17.7 M | train\n",
      "1 | loss_fn | MSELoss    | 0      | train\n",
      "-----------------------------------------------\n",
      "17.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.7 M    Total params\n",
      "70.839    Total estimated model params size (MB)\n",
      "149       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12af317e03a47e7862e140f925f4b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zak/anaconda3/envs/DataCamp/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/zak/anaconda3/envs/DataCamp/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/zak/anaconda3/envs/DataCamp/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d8eaf5fcfc429e98a0482fd6047d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653dd296b11a455683ce650139354b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078ba0f84bd74f668fafb60453f51bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe92b77b572443d4b93b475a193571f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6662874d427f440a9fbfb43e1c22a63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0189e7822d4d83bca2b757f5db4a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484b6a0cb30b494eb5ba4994f41f394c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b677a4a80c4776acbacb5fac3dec7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac9906727fe4d94b6c00fbecd4d5025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d47829760fe4a409c74b437668443fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb65ab0e2604763935cb666a5a589e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1510b91c12459ba1ee20dd83c69859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1aeec892c0547c3a0a4abba8d1385e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a491f9fccf40ec80f19f5afeb3fc35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a929fb0d6ede4d3894c241d44ef14ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfd6a4713154685a9d4c4270dd78e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fa51fe150640f1b0b4e3e6dd2d1608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78aec577660c4cd2bfcd8b1d3493f892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e37004a3494402857dda1aaad1a0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c015332017dc4fbbacfc456557a57531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00051bfdc49745048af8d3577675319e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 20-epoch training complete.\n"
     ]
    }
   ],
   "source": [
    "# Simplified CPU training cell (20 epochs) with metrics recorder callback\n",
    "import torch, pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class EpochMetricsRecorder(pl.Callback):\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        cm = trainer.callback_metrics\n",
    "        # Extract metrics if present\n",
    "        entry = {\n",
    "            'epoch': trainer.current_epoch + 1,\n",
    "            'train_loss': float(cm.get('train_loss', float('nan'))),\n",
    "            'val_loss': float(cm.get('val_loss', float('nan'))),\n",
    "            'train_wape': float(cm.get('train_wape', float('nan'))),\n",
    "            'val_wape': float(cm.get('val_wape', float('nan'))),\n",
    "            'train_accuracy': float(cm.get('train_accuracy', float('nan'))),\n",
    "            'val_accuracy': float(cm.get('val_accuracy', float('nan'))),\n",
    "            'train_mae': float(cm.get('train_mae', float('nan'))),\n",
    "            'val_mae': float(cm.get('val_mae', float('nan')))\n",
    "        }\n",
    "        self.history.append(entry)\n",
    "\n",
    "print('\\n=== CPU Training (20 epochs) ===')\n",
    "print(f'Device: {device} | Epochs: {EPOCHS} | Batch size: {BATCH_SIZE}')\n",
    "assert device.type == 'cpu', 'Device not CPU; run the environment cell first.'\n",
    "\n",
    "model.train().to(device)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "recorder = EpochMetricsRecorder()\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, accelerator='cpu', devices=1, log_every_n_steps=5, enable_progress_bar=True, callbacks=[recorder])\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "print('CPU 20-epoch training complete. Recorded', len(recorder.history), 'epochs of metrics.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c290fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 2.1796 | MAE: 1.1598 | WAPE: 7.27%\n"
     ]
    }
   ],
   "source": [
    "# Plot training & validation loss and accuracy vs epochs\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "if 'recorder' not in globals() or not getattr(recorder, 'history', None):\n",
    "    print('No recorded history found. Re-run training cell with EpochMetricsRecorder.')\n",
    "else:\n",
    "    df = pd.DataFrame(recorder.history)\n",
    "    display(df.tail())\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "    axes[0].plot(df['epoch'], df['train_loss'], label='Train Loss')\n",
    "    axes[0].plot(df['epoch'], df['val_loss'], label='Val Loss')\n",
    "    axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].set_title('Loss vs Epoch'); axes[0].legend(); axes[0].grid(alpha=0.3)\n",
    "    axes[1].plot(df['epoch'], df['train_accuracy'], label='Train Accuracy')\n",
    "    axes[1].plot(df['epoch'], df['val_accuracy'], label='Val Accuracy')\n",
    "    axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Accuracy (%)'); axes[1].set_title('Accuracy (100 - WAPE) vs Epoch'); axes[1].legend(); axes[1].grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a3e496f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint -> artifacts/models/nbeats_notebook.ckpt\n",
      "Saved metrics -> artifacts/models/nbeats_notebook_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Save artifacts\n",
    "CKPT_PATH = ARTIFACTS_DIR / 'nbeats_notebook.ckpt'\n",
    "torch.save(model.state_dict(), CKPT_PATH)\n",
    "metrics = {\n",
    "    'validation_mse': avg_loss,\n",
    "    'validation_mae': avg_mae,\n",
    "    'validation_wape': wape,\n",
    "    'config': cfg_model.__dict__,\n",
    "    'n_train_windows': len(train_ds),\n",
    "    'n_val_windows': len(val_ds),\n",
    "}\n",
    "with open(ARTIFACTS_DIR / 'nbeats_notebook_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print('Saved checkpoint ->', CKPT_PATH)\n",
    "print('Saved metrics ->', ARTIFACTS_DIR / 'nbeats_notebook_metrics.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc149ce9",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Integrate with backtesting harness.\n",
    "- Add basis (trend/seasonality) blocks for improved decomposition.\n",
    "- Introduce quantile heads for probabilistic forecasts.\n",
    "- Add per-item embeddings & categorical covariates.\n",
    "- Promote best checkpoint to API service for live forecasts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataCamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
